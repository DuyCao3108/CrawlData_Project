{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from lxml import etree\n",
    "import math\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | DEFINING FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 | REQUESTING FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Em xem trên mạng người ta chỉ có cái web \"scrapeops\" cho mình fake user agents. \n",
    "    Nhưng mà nhiều web vẫn không fake được vì còn nhiều cái khác block mình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents():\n",
    "    response = requests.get(\n",
    "                        url='https://headers.scrapeops.io/v1/user-agents',\n",
    "                        params={\n",
    "                            'api_key': 'c77ada02-6767-4b0e-bd5d-38aea43c9431',\n",
    "                            'num_results': '2'}\n",
    "                        )\n",
    "    agents = response.json()['result']\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url, agents):\n",
    "    for agent in agents:\n",
    "        headers = {'User-Agent': agent}\n",
    "        r = urllib.request.Request(url, headers= headers)\n",
    "        html = urllib.request.urlopen(r).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 | FUNCTION THAT EXTRACTS DATA FROM 1 URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2 function chỗ này dùng để:\n",
    "    - get_job_soups: \n",
    "        Mục đích: lấy tất cả URL mà thông tin chi tiết của các job được để\n",
    "        Arguments:\n",
    "            soup: soup object của page được hiển thị sau khi mình search lần đầu trên web, là base url\n",
    "            website: mỗi web lấy mỗi khác nên em dùng arg website để tách cách xử lý input\n",
    "        Output: các soup objects của những page thông tin chi tiết của các job\n",
    "    - get_infor_from_job_soups:\n",
    "        Mục đích: lấy tất cả thông tin cần thiết từ page thông tin chi tiết của mỗi job post\n",
    "        Arguments:\n",
    "            job_soups: output của get_job_soups, là 1 list các soup objects của các page job\n",
    "            website: mỗi web lấy mỗi khác nên em dùng arg website để tách cách xử lý input\n",
    "        Output: thông tin chi tiết của từng job, dưới dạng json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get link to job items \n",
    "def get_job_soups(soup, domain_url, website):\n",
    "    if website == 'ITVIEC':\n",
    "        divs = soup.find_all('div', class_ = 'job-card ipt-2 d-flex flex-column border-radius-large position-relative bg-white')\n",
    "        job_links = [div.select_one('h3.imt-3 > a')['href'] for div in divs]\n",
    "        job_links = [urljoin(domain_url, job_link)for job_link in job_links]\n",
    "        job_soups = [extract(url=job_link, agents=get_agents()) for job_link in job_links]\n",
    "    elif website == 'VIECLAM24H':\n",
    "        divs = soup.find_all('div', class_ = 'relative lg:h-[115px] w-full flex rounded-sm lg:mb-3 mb-2 lg:hover:shadow-md')\n",
    "        job_links = [div.select_one(\"a\")['href'] for div in divs]\n",
    "        job_links = [urljoin(domain_url, job_link)for job_link in job_links]\n",
    "        job_soups = [extract(url=job_link, agents=get_agents()) for job_link in job_links]\n",
    "    elif website == 'JOBSGO':\n",
    "        divs = soup.find_all('article', class_ = 'article')\n",
    "        job_links = [div.select_one('a')['href'] for div in divs]\n",
    "        job_soups = [extract(url=job_link, agents=get_agents()) for job_link in job_links]\n",
    "    elif website == 'JOBOKO':\n",
    "        job_links = [s['href'] for s in soup.select(\"div[class='item-head'] > h2 > a\")]\n",
    "        job_links = [urljoin(domain_url, job_link)for job_link in job_links]\n",
    "        job_soups = [extract(url=job_link, agents=get_agents()) for job_link in job_links]\n",
    "    return job_soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from job page\n",
    "def get_infor_from_job_soups(job_soups, website):\n",
    "    if website == 'ITVIEC':\n",
    "        data_list = []\n",
    "        for soup in job_soups:\n",
    "            data = {\n",
    "                'JobTitle': soup.select_one('div.job-header-info > h1').text.strip(),\n",
    "                'Location': soup.find_all('span', class_ ='normal-text text-rich-grey')[0].text.strip(),\n",
    "                'PostDate': soup.select_one(\"div[class='d-inline-block text-dark-grey preview-header-item'] > span[class='normal-text text-rich-grey']\").text.strip(),\n",
    "                'JobDescription': [s.text.strip() for s in soup.select(\"div[class='imy-5 paragraph']\")],\n",
    "                'Tools': [s.text.strip() for s in soup.select(\"div[class='imt-2'] > a > div\")]\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    elif website == 'VIECLAM24H':\n",
    "        data_list = []\n",
    "        for soup in job_soups:\n",
    "            data = {\n",
    "                'JobTitle': soup.select_one(\"div[class='md:ml-7 w-full'] > h1\").text.strip(),\n",
    "                'Location': soup.select_one(\"div[class='ml-3 text-14 md:flex pt-0 md:pt-[5px]'] > p > a\")['title'],\n",
    "                'PostDate': soup.select_one(\"div[class='ml-3'] > p[class='text-14']\").text.strip(),\n",
    "                'JobDescription': [s.text.strip() for s in soup.select(\"div[class='jsx-d84db6a84feb175e mb-2 text-14 break-words text-se-neutral-80 text-description']\")],\n",
    "                'Level': re.findall(\"Cấp bậc(.*)\", string=soup.select_one(\"i[class='w-[32px] h-[32px] min-w-[32px] flex items-center justify-center rounded-full bg-[#EDE9FE] text-[#8B5CF6] text-20 svicon-medal'] + div\").text.strip()),\n",
    "                'ExperienceRequirement': re.findall(\"Yêu cầu kinh nghiệm(.*)\", string=soup.select_one(\"i[class='w-[32px] h-[32px] min-w-[32px] flex items-center justify-center rounded-full bg-[#EDE9FE] text-[#8B5CF6] text-20 svicon-experience-user'] + div\").text.strip()),\n",
    "                'SalaryRange': re.findall(\"Mức lương : (.*)\",string=soup.select_one(\"div[class='flex items-start min-w-[250px] mb-4']>div[class='ml-3 text-14 md:flex pt-0 md:pt-[5px]']\").text.strip()),\n",
    "                'CompanyName': soup.select_one(\"div[class='px-4 md:px-10 py-4 bg-white shadow-sd-12 rounded-sm mt-4 lg:mb-6'] > a > h3\").text.strip(),\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    elif website == 'JOBSGO':\n",
    "        data_list = []\n",
    "        for soup in job_soups:\n",
    "            data = {\n",
    "                'JobTitle': soup.select_one(\"div[class='media-body-2'] > h1\").text.strip(),\n",
    "                'Location': soup.select_one(\"div[class='content-group'] > div[class='box-jobs-info'] > div p\").text.strip(),\n",
    "                'PostDate_Level_ExperienceRequirement': \"\\n\".join([s.text.strip() for s in soup.select(\"div[class='col-sm-4 col-xs-6'] > p\")]),\n",
    "                'JobDescription': [s.text.strip() for s in soup.find_all(\"div\", class_=\"clearfix\")],\n",
    "                'SalaryRange': re.findall(\"Mức lương (.*)\", string = \"\\n\".join([s.text.strip() for s in soup.select(\"div[class='media-body-2'] li\")])),\n",
    "                'CompanyName': soup.select_one(\"div[class='profile-cover'] div[class='media-body'] > h2 > a\").text.strip() if soup.select_one(\"div[class='profile-cover'] div[class='media-body'] > h2 > a\") else \"\"\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    elif website == 'JOBOKO':\n",
    "        data_list = []\n",
    "        for soup in job_soups:\n",
    "            data = {\n",
    "                'JobTitle': soup.select_one(\"h2[class='fz-20 fw-bold letter-spacing-1 text-uppercase c-green nw-company-hero__title']\").text.strip(),\n",
    "                'Location': re.findall(\"Địa điểm làm việc: (.*)\", string=soup.select_one(\"div[class='nw-company-hero__address'] > span\").text.strip()),\n",
    "                'JobDescription': [s.text.strip() for s in soup.select(\"div[class='fz-15 block-text c-text-2'] > div[class='text-justify']\")],\n",
    "                'ExperienceRequirement': re.findall(\"Kinh nghiệm: (.*) năm\", \"\\n\".join([s.text.strip() for s in soup.select(\"div[class='fz-15 letter-spacing-1 item-content']\")])),\n",
    "                'SalaryRange': re.findall(\"Mức lương: (.*)\", soup.select_one(\"div[class='col-12'] div[class='fz-15 letter-spacing-1 item-content']\").text.strip()), \n",
    "                'Level': re.findall(\"Chức vụ: (.*)\" , \"\\n\".join([s.text.strip() for s in soup.select(\"div[class='fz-15 letter-spacing-1 item-content']\")])),\n",
    "                'CompanyName': soup.select_one(\"div[class='nw-company-hero__info'] > a\").text.strip() if soup.select_one(\"div[class='nw-company-hero__info'] > a\") else soup.select_one(\"div[class='nw-company-hero__info'] > div[class='fz-16 letter-spacing-1 text-capitalize c-text-2 nw-company-hero__text']\").text.strip()\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    function \"extract_data_from_curr_url\" là để gộp 2 function phía trên vào chung để thực hiện cùng 1 lần\n",
    "    process:\n",
    "    - nó lấy url đầu tiên mà mình search (\"current url\")\n",
    "    - convert page thành soup object\n",
    "    - lấy soup objects của các page thông tin chi tiết của các job\n",
    "    - lấy thông tin chi tiết của các job post\n",
    "    - trả về json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine get_job_soups > get_infor_from_job_soups\n",
    "def extract_data_from_curr_url(domain_url, current_url, website):\n",
    "    soup = extract(url=current_url, agents = get_agents())\n",
    "    job_soups = get_job_soups(soup, domain_url, website)\n",
    "    data_list = get_infor_from_job_soups(job_soups, website)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 | FUNCTION THAT GETS ALL URLS ASSOCIATED WITH 1 URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    function \"get_all_next_pages_url\" dùng để lấy tất cả url liên kết với url gốc của mình, dùng để cào từ page này sang page khác\n",
    "    mỗi website process khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all next pages url to enable spider\n",
    "def get_all_next_pages_url(curr_soup, domain_url, website, query):\n",
    "    if website == 'ITVIEC':\n",
    "        all_next_pages_url = []\n",
    "        while curr_soup.select_one('div[class=\"page next\"]') is not None:\n",
    "            next_url = curr_soup.select_one('div[class=\"page next\"] >a')['href']\n",
    "            full_next_url = urljoin(domain_url, next_url)\n",
    "            all_next_pages_url.append(full_next_url)\n",
    "            curr_soup = extract(full_next_url, agents = get_agents())\n",
    "    elif website == 'VIECLAM24H':\n",
    "        # round up the number of pages\n",
    "        pages = math.ceil(int(curr_soup.select_one(\"div[class='flex items-center'] span[class='font-semibold']\").text.strip()) / 30)\n",
    "        all_next_pages_url = []\n",
    "        for page in range(2, pages + 1): # to get correct next pages\n",
    "            url = \"https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page={}&q={}&sort_q=\".format(str(page),query)\n",
    "            all_next_pages_url.append(url)\n",
    "    elif website == 'JOBSGO':\n",
    "        # round up the number of pages\n",
    "        pages = math.ceil(int(re.findall(\".* / (.*)\", string = curr_soup.select_one(\"div[class='sidebar-widget-title mrg-bot-15'] > span\").text.strip())[0]) / 50)\n",
    "        all_next_pages_url = []\n",
    "        for page in range(2, pages + 1): # to get correct next pages\n",
    "            url = \"https://jobsgo.vn/viec-lam-{}-tai-ho-chi-minh.html?page={}\".format(query, str(page))\n",
    "            all_next_pages_url.append(url)\n",
    "    elif website == 'JOBOKO':\n",
    "        all_next_pages_url = []\n",
    "        pages = math.ceil(int(\"\".join(re.findall(\"[0-9]\", curr_soup.select_one(\"li[class='nav-item'] >a>span\").text))) / 10)\n",
    "        true_pages = 50 if pages >= 50 else pages\n",
    "        for page in range(2, true_pages+1):\n",
    "            url = \"https://vn.joboko.com/jobs?q={}&p={} \".format(query,page)\n",
    "            all_next_pages_url.append(url)\n",
    "    return all_next_pages_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 | FUNCTION THAT CRAWLS THROUGH ALL THOSE URLS AND GET DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    function \"crawl_data_from_url\" cào dữ liệu từ tất cả page\n",
    "    nhận input là url đầu tiên, website đang cào, và query search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL FUNCTION THAT CRAWLS THROUGH MULTIPLE PAGES TO EXTRACT RELEVANT DATA\n",
    "def crawl_data_from_url(domain_url, current_url, website, query):\n",
    "    # get all pages urls\n",
    "    curr_soup = extract(current_url, agents = get_agents())\n",
    "    all_next_pages_url = get_all_next_pages_url(curr_soup, domain_url, website = website, query = query)\n",
    "    print('got all urls!')\n",
    "    print(all_next_pages_url)\n",
    "    # initiate first url\n",
    "    data_list_of_all_urls = extract_data_from_curr_url(domain_url, current_url = current_url, website = website)\n",
    "    for page_url in all_next_pages_url:\n",
    "        data_list_cur_url = extract_data_from_curr_url(domain_url, current_url = page_url, website = website)\n",
    "        for data_list in data_list_cur_url:\n",
    "            data_list_of_all_urls.append(data_list)\n",
    "        print(\"extracted data from {}\".format(page_url))\n",
    "    return all_next_pages_url, data_list_of_all_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | SAMPLE RUN ON SOME WEBSITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got all urls!\n",
      "['https://jobsgo.vn/viec-lam-data-tai-ho-chi-minh.html?page=2']\n",
      "extracted data from https://jobsgo.vn/viec-lam-data-tai-ho-chi-minh.html?page=2\n"
     ]
    }
   ],
   "source": [
    "all_next_pages_url, data_list_of_all_urls = crawl_data_from_url(\n",
    "    domain_url = 'https://jobsgo.vn/', \n",
    "    current_url = 'https://jobsgo.vn/viec-lam-data.html', \n",
    "    website = 'JOBSGO', \n",
    "    query = 'data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got all urls!\n",
      "['https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=2&q=IT&sort_q=', 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=3&q=IT&sort_q=', 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=4&q=IT&sort_q=', 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=5&q=IT&sort_q=', 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=6&q=IT&sort_q=', 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=7&q=IT&sort_q=']\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=2&q=IT&sort_q=\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=3&q=IT&sort_q=\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=4&q=IT&sort_q=\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=5&q=IT&sort_q=\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=6&q=IT&sort_q=\n",
      "extracted data from https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?page=7&q=IT&sort_q=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>Location</th>\n",
       "      <th>PostDate</th>\n",
       "      <th>JobDescription</th>\n",
       "      <th>Level</th>\n",
       "      <th>ExperienceRequirement</th>\n",
       "      <th>SalaryRange</th>\n",
       "      <th>CompanyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nhân Viên IT- IT Helpdesk</td>\n",
       "      <td>Việc làm tại Bình Dương</td>\n",
       "      <td>20/12/2023</td>\n",
       "      <td>[-Xác định và giải quyết các vấn đề liên quan ...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[Chưa có kinh nghiệm]</td>\n",
       "      <td>[7 - 10 triệu]</td>\n",
       "      <td>Công ty TNHH Liên Doanh Hasan - Dermapharm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nhân Viên IT</td>\n",
       "      <td>Việc làm tại TP.HCM</td>\n",
       "      <td>25/11/2023</td>\n",
       "      <td>[- Cập nhật sản phẩm kinh doanh của công ty lê...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[Dưới 1 năm]</td>\n",
       "      <td>[10 - 15 triệu]</td>\n",
       "      <td>Công Ty TNHH Đặng Mậu Tấn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chuyên Viên IT Helpdesk Tại Kim Mã, Hà Nội</td>\n",
       "      <td>Việc làm tại Hà Nội</td>\n",
       "      <td>15/12/2023</td>\n",
       "      <td>[- Xử lý các sự cố kỹ thuật liên quan đến hệ t...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[Dưới 1 năm]</td>\n",
       "      <td>[9 - 15 triệu]</td>\n",
       "      <td>Công ty Tài Chính TNHH HD SAISON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Giáo Viên Tin Học - IT</td>\n",
       "      <td>Việc làm tại TP.HCM</td>\n",
       "      <td>11/12/2023</td>\n",
       "      <td>[- Giảng dạy theo chương trình của Bộ Giáo Dục...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[Chưa có kinh nghiệm]</td>\n",
       "      <td>[6 - 12 triệu]</td>\n",
       "      <td>Công Ty TNHH Một Thành Viên Vi Tính Nt Đông</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nhân Viên IT Help Desk</td>\n",
       "      <td>Việc làm tại TP.HCM</td>\n",
       "      <td>25/11/2023</td>\n",
       "      <td>[-       Set up các phòng học vi tính, hệ thốn...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[Chưa có kinh nghiệm]</td>\n",
       "      <td>[7 - 15 triệu]</td>\n",
       "      <td>Công Ty TNHH Tư Vấn &amp; Đào Tạo Đại Dương</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Sale Manager (Thiết Bị Mã Vạch, Pos)</td>\n",
       "      <td>Việc làm tại TP.HCM</td>\n",
       "      <td>24/11/2023</td>\n",
       "      <td>[Chịu trách nhiệm về chỉ tiêu doanh số được gi...</td>\n",
       "      <td>[Quản lý cấp cao]</td>\n",
       "      <td>[3 năm]</td>\n",
       "      <td>[Thoả thuận]</td>\n",
       "      <td>Công Ty Cổ Phần Thương Mại Dịch Vụ Tín Hòa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Nhân Viên Kỹ Thuật Dự Án - Mảng Công Nghệ Thôn...</td>\n",
       "      <td>Việc làm tại Hà Nội</td>\n",
       "      <td>24/11/2023</td>\n",
       "      <td>[- Tư vấn giải pháp cho khách hàng;- Lập báo g...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[1 năm]</td>\n",
       "      <td>[13 - 20 triệu]</td>\n",
       "      <td>Công Ty TNHH Công Nghệ Nguyên Lộc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Trưởng Phòng Kỹ Thuật</td>\n",
       "      <td>Việc làm tại Hà Nội</td>\n",
       "      <td>24/11/2023</td>\n",
       "      <td>[Quản lý, điều hành các hoạt động phòng kỹ thu...</td>\n",
       "      <td>[Quản lý cấp trung]</td>\n",
       "      <td>[3 năm]</td>\n",
       "      <td>[18 - 25 triệu]</td>\n",
       "      <td>Công Ty TNHH Thương Mại Điện Tử Bảo Giang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Sóc Trăng Nhân Viên Kỹ Thuật</td>\n",
       "      <td>Việc làm tại Sóc Trăng</td>\n",
       "      <td>24/11/2023</td>\n",
       "      <td>[- Cài đặt thiết bị, phần mềm của công ty.- Hỗ...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[1 năm]</td>\n",
       "      <td>[9 - 13 triệu]</td>\n",
       "      <td>Công ty Cổ Phần Vacxin Việt Nam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Bạc Liêu- Nhân Viên Bảo Trì</td>\n",
       "      <td>Việc làm tại Bạc Liêu</td>\n",
       "      <td>24/11/2023</td>\n",
       "      <td>[- Cài đặt thiết bị, phần mềm của công ty.- Hỗ...</td>\n",
       "      <td>[Chuyên viên- nhân viên]</td>\n",
       "      <td>[1 năm]</td>\n",
       "      <td>[9 - 13 triệu]</td>\n",
       "      <td>Công ty Cổ Phần Vacxin Việt Nam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              JobTitle  \\\n",
       "0                            Nhân Viên IT- IT Helpdesk   \n",
       "1                                         Nhân Viên IT   \n",
       "2           Chuyên Viên IT Helpdesk Tại Kim Mã, Hà Nội   \n",
       "3                               Giáo Viên Tin Học - IT   \n",
       "4                               Nhân Viên IT Help Desk   \n",
       "..                                                 ...   \n",
       "200               Sale Manager (Thiết Bị Mã Vạch, Pos)   \n",
       "201  Nhân Viên Kỹ Thuật Dự Án - Mảng Công Nghệ Thôn...   \n",
       "202                              Trưởng Phòng Kỹ Thuật   \n",
       "203                       Sóc Trăng Nhân Viên Kỹ Thuật   \n",
       "204                        Bạc Liêu- Nhân Viên Bảo Trì   \n",
       "\n",
       "                    Location    PostDate  \\\n",
       "0    Việc làm tại Bình Dương  20/12/2023   \n",
       "1        Việc làm tại TP.HCM  25/11/2023   \n",
       "2        Việc làm tại Hà Nội  15/12/2023   \n",
       "3        Việc làm tại TP.HCM  11/12/2023   \n",
       "4        Việc làm tại TP.HCM  25/11/2023   \n",
       "..                       ...         ...   \n",
       "200      Việc làm tại TP.HCM  24/11/2023   \n",
       "201      Việc làm tại Hà Nội  24/11/2023   \n",
       "202      Việc làm tại Hà Nội  24/11/2023   \n",
       "203   Việc làm tại Sóc Trăng  24/11/2023   \n",
       "204    Việc làm tại Bạc Liêu  24/11/2023   \n",
       "\n",
       "                                        JobDescription  \\\n",
       "0    [-Xác định và giải quyết các vấn đề liên quan ...   \n",
       "1    [- Cập nhật sản phẩm kinh doanh của công ty lê...   \n",
       "2    [- Xử lý các sự cố kỹ thuật liên quan đến hệ t...   \n",
       "3    [- Giảng dạy theo chương trình của Bộ Giáo Dục...   \n",
       "4    [-       Set up các phòng học vi tính, hệ thốn...   \n",
       "..                                                 ...   \n",
       "200  [Chịu trách nhiệm về chỉ tiêu doanh số được gi...   \n",
       "201  [- Tư vấn giải pháp cho khách hàng;- Lập báo g...   \n",
       "202  [Quản lý, điều hành các hoạt động phòng kỹ thu...   \n",
       "203  [- Cài đặt thiết bị, phần mềm của công ty.- Hỗ...   \n",
       "204  [- Cài đặt thiết bị, phần mềm của công ty.- Hỗ...   \n",
       "\n",
       "                        Level  ExperienceRequirement      SalaryRange  \\\n",
       "0    [Chuyên viên- nhân viên]  [Chưa có kinh nghiệm]   [7 - 10 triệu]   \n",
       "1    [Chuyên viên- nhân viên]           [Dưới 1 năm]  [10 - 15 triệu]   \n",
       "2    [Chuyên viên- nhân viên]           [Dưới 1 năm]   [9 - 15 triệu]   \n",
       "3    [Chuyên viên- nhân viên]  [Chưa có kinh nghiệm]   [6 - 12 triệu]   \n",
       "4    [Chuyên viên- nhân viên]  [Chưa có kinh nghiệm]   [7 - 15 triệu]   \n",
       "..                        ...                    ...              ...   \n",
       "200         [Quản lý cấp cao]                [3 năm]     [Thoả thuận]   \n",
       "201  [Chuyên viên- nhân viên]                [1 năm]  [13 - 20 triệu]   \n",
       "202       [Quản lý cấp trung]                [3 năm]  [18 - 25 triệu]   \n",
       "203  [Chuyên viên- nhân viên]                [1 năm]   [9 - 13 triệu]   \n",
       "204  [Chuyên viên- nhân viên]                [1 năm]   [9 - 13 triệu]   \n",
       "\n",
       "                                     CompanyName  \n",
       "0     Công ty TNHH Liên Doanh Hasan - Dermapharm  \n",
       "1                      Công Ty TNHH Đặng Mậu Tấn  \n",
       "2               Công ty Tài Chính TNHH HD SAISON  \n",
       "3    Công Ty TNHH Một Thành Viên Vi Tính Nt Đông  \n",
       "4        Công Ty TNHH Tư Vấn & Đào Tạo Đại Dương  \n",
       "..                                           ...  \n",
       "200   Công Ty Cổ Phần Thương Mại Dịch Vụ Tín Hòa  \n",
       "201            Công Ty TNHH Công Nghệ Nguyên Lộc  \n",
       "202    Công Ty TNHH Thương Mại Điện Tử Bảo Giang  \n",
       "203              Công ty Cổ Phần Vacxin Việt Nam  \n",
       "204              Công ty Cổ Phần Vacxin Việt Nam  \n",
       "\n",
       "[205 rows x 8 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_next_pages_url, data_list_of_all_urls = crawl_data_from_url(\n",
    "    domain_url = 'https://vieclam24h.vn/', \n",
    "    current_url = 'https://vieclam24h.vn/tim-kiem-viec-lam-nhanh?q=IT', \n",
    "    website = 'VIECLAM24H', \n",
    "    query = 'IT'\n",
    ")\n",
    "\n",
    "df_VIECLAM24H = pd.DataFrame(data_list_of_all_urls)\n",
    "df_VIECLAM24H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VIECLAM24H.to_csv(r\"C:\\Users\\Admin\\airflow1412\\csvfile\\VIECLAM24H.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
